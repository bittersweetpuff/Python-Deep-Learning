{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorium 5: Rekurencyjne Sieci Neuronowe (RNN)\n",
    "## Część 1: number2roman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import RNN, LSTM, RepeatVector, Dropout\n",
    "import numpy as np\n",
    "from roman_numerals import convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tworzenie zestawu danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "Sample (input): 0\n",
      "Label []\n",
      "Label encoded (output):\n",
      " [[0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]]\n",
      "(200, 9, 6)\n"
     ]
    }
   ],
   "source": [
    "MAX_OUTPUT_SEQUENCE_LEN=0\n",
    "DATASET_SIZE=200\n",
    "\n",
    "samples = []\n",
    "labels = []\n",
    "\n",
    "import random\n",
    "\n",
    "for i in range(DATASET_SIZE):\n",
    "    samples.append(i)\n",
    "    roman = convert(i)\n",
    "    labels.append(list(roman))\n",
    "    if len(roman) > MAX_OUTPUT_SEQUENCE_LEN:\n",
    "        MAX_OUTPUT_SEQUENCE_LEN = len(roman)\n",
    "    \n",
    "print(MAX_OUTPUT_SEQUENCE_LEN)\n",
    "samples = np.array(samples)\n",
    "labels = np.array(labels, dtype=object)\n",
    "\n",
    "print(\"Sample (input):\",samples[0])\n",
    "print(\"Label\",labels[0])\n",
    "\n",
    "\n",
    "codes = ' IVXLC'\n",
    "\n",
    "nlabels = np.zeros((DATASET_SIZE,MAX_OUTPUT_SEQUENCE_LEN,len(codes)))\n",
    "for i in range(DATASET_SIZE):\n",
    "    for j in range(MAX_OUTPUT_SEQUENCE_LEN):\n",
    "        if j>=len(labels[i]): \n",
    "                nlabels[i][j][0]=1\n",
    "                continue\n",
    "        x = labels[i][j]\n",
    "        index = codes.index(x)\n",
    "        nlabels[i][j][index] = 1\n",
    "print(\"Label encoded (output):\\n\",nlabels[123])\n",
    "labels = nlabels\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podział na zestaw uczący i trenujący"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 100  test samples 100\n"
     ]
    }
   ],
   "source": [
    "TRAINING_SIZE = .5\n",
    "from sklearn.model_selection import train_test_split\n",
    "(trainSamples, testSamples, trainLabels, testLabels) = train_test_split(samples, labels,train_size=TRAINING_SIZE)\n",
    "print('Training samples:',len(trainSamples),' test samples',len(testSamples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tworzenie sieci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 9, 16)             0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 9, 128)            74240     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 9, 128)            131584    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 9, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 9, 128)            131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 9, 6)              774       \n",
      "=================================================================\n",
      "Total params: 338,214\n",
      "Trainable params: 338,214\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=1) ) \n",
    "model.add(RepeatVector(9)) #length of the text\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dense(len(codes),activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\",metrics=['accuracy', 'mae'])\n",
    "num_epochs = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 100 samples 50 epochs and batch_size= 50\n",
      "Epochs so far 0\n",
      "\n",
      "Epoch 50 - loss = 0.783, loss improvement = 0.909\n",
      "186 -> LXXXII\n",
      "63 -> XXXI\n",
      "124 -> XXXII\n",
      "61 -> XXXI\n",
      "51 -> XXXI\n",
      "Correct 4 of 200  =  0.02\n",
      "\n",
      "Epoch 100 - loss = 0.701, loss improvement = 0.081\n",
      "195 -> CLXXXI\n",
      "55 -> XXXI\n",
      "18 -> XXI\n",
      "63 -> XXXI\n",
      "145 -> CXXXI\n",
      "Correct 6 of 200  =  0.03\n",
      "\n",
      "Epoch 150 - loss = 0.639, loss improvement = 0.066\n",
      "168 -> CLXXI\n",
      "100 -> CXXI\n",
      "55 -> XXXI\n",
      "25 -> XXII\n",
      "55 -> XXXI\n",
      "Correct 8 of 200  =  0.04\n",
      "\n",
      "Epoch 200 - loss = 0.585, loss improvement = 0.052\n",
      "19 -> XXII\n",
      "7 -> II\n",
      "125 -> CXXII\n",
      "60 -> LXVI\n",
      "181 -> CLXXXI\n",
      "Correct 9 of 200  =  0.045\n",
      "\n",
      "Epoch 250 - loss = 0.529, loss improvement = 0.052\n",
      "6 -> I\n",
      "37 -> XXXI\n",
      "35 -> XXXI\n",
      "172 -> CLXXII\n",
      "103 -> CXXI\n",
      "Correct 10 of 200  =  0.05\n",
      "\n",
      "Epoch 300 - loss = 0.476, loss improvement = 0.052\n",
      "120 -> CXXI\n",
      "105 -> CXII\n",
      "145 -> CXLII\n",
      "137 -> CXXII\n",
      "81 -> LXXXI\n",
      "Correct 17 of 200  =  0.085\n",
      "\n",
      "Epoch 350 - loss = 0.491, loss improvement =-0.019\n",
      "161 -> CLXII\n",
      "167 -> CLXXII\n",
      "92 -> XCII\n",
      "104 -> CXII\n",
      "92 -> XCII\n",
      "Correct 21 of 200  =  0.105\n",
      "\n",
      "Epoch 400 - loss = 0.420, loss improvement = 0.031\n",
      "112 -> CXII\n",
      "7 -> I\n",
      "46 -> XLVI\n",
      "129 -> CXXXI\n",
      "171 -> CLXXII\n",
      "Correct 24 of 200  =  0.12\n",
      "\n",
      "Epoch 450 - loss = 0.405, loss improvement = 0.014\n",
      "18 -> XVII\n",
      "104 -> CCI\n",
      "173 -> CLXXII\n",
      "182 -> CLXXXI\n",
      "72 -> LXXII\n",
      "Correct 22 of 200  =  0.11\n",
      "\n",
      "Epoch 500 - loss = 0.360, loss improvement = 0.034\n",
      "110 -> CXII\n",
      "194 -> CLXXXI\n",
      "69 -> LXXII\n",
      "4 -> IV\n",
      "64 -> LXVI\n",
      "Correct 23 of 200  =  0.115\n",
      "\n",
      "Epoch 550 - loss = 0.362, loss improvement = 0.006\n",
      "45 -> XLI\n",
      "40 -> XLXI\n",
      "193 -> CLXIX\n",
      "32 -> XXXI\n",
      "183 -> CLXXXII\n",
      "Correct 28 of 200  =  0.14\n",
      "\n",
      "Epoch 600 - loss = 0.333, loss improvement = 0.015\n",
      "75 -> LXXVI\n",
      "113 -> CXII\n",
      "37 -> XXXVI\n",
      "29 -> XXIX\n",
      "138 -> CXXXII\n",
      "Correct 35 of 200  =  0.175\n",
      "\n",
      "Epoch 650 - loss = 0.319, loss improvement = 0.020\n",
      "127 -> CXXXI\n",
      "94 -> XCV\n",
      "125 -> CXXII\n",
      "114 -> CXVI\n",
      "63 -> LXVI\n",
      "Correct 36 of 200  =  0.18\n",
      "\n",
      "Epoch 700 - loss = 0.326, loss improvement =-0.007\n",
      "162 -> CLXII\n",
      "23 -> XXII\n",
      "50 -> XIV\n",
      "44 -> XLVI\n",
      "54 -> LII\n",
      "Correct 39 of 200  =  0.195\n",
      "\n",
      "Epoch 750 - loss = 0.323, loss improvement = 0.021\n",
      "55 -> LVI\n",
      "191 -> CLCI\n",
      "47 -> XLVII\n",
      "77 -> LXXVI\n",
      "15 -> XIII\n",
      "Correct 38 of 200  =  0.19\n",
      "\n",
      "Epoch 800 - loss = 0.281, loss improvement = 0.076\n",
      "78 -> LXXVII\n",
      "33 -> XXXII\n",
      "38 -> XXXVII\n",
      "96 -> XCVI\n",
      "124 -> CXXI\n",
      "Correct 46 of 200  =  0.23\n",
      "\n",
      "Epoch 850 - loss = 0.290, loss improvement = 0.015\n",
      "3 -> III\n",
      "21 -> XX\n",
      "54 -> LII\n",
      "107 -> CIII\n",
      "198 -> CLCI\n",
      "Correct 46 of 200  =  0.23\n",
      "\n",
      "Epoch 900 - loss = 0.313, loss improvement =-0.011\n",
      "105 -> CVI\n",
      "70 -> LXVI\n",
      "179 -> CLXXII\n",
      "150 -> CXLV\n",
      "198 -> CLXI\n",
      "Correct 43 of 200  =  0.215\n",
      "\n",
      "Epoch 950 - loss = 0.268, loss improvement = 0.101\n",
      "23 -> XXIVI\n",
      "93 -> XCVII\n",
      "134 -> CXXXII\n",
      "111 -> CXVI\n",
      "55 -> LII\n",
      "Correct 40 of 200  =  0.2\n",
      "\n",
      "Epoch 1000 - loss = 0.248, loss improvement = 0.050\n",
      "44 -> XLIV\n",
      "192 -> CLCI\n",
      "107 -> CVII\n",
      "54 -> LII\n",
      "107 -> CVII\n",
      "Correct 49 of 200  =  0.245\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def label2words(label):\n",
    "    s = ''\n",
    "    for r in label:\n",
    "        s+=codes[int(r)]\n",
    "        #print(i,'->',s)\n",
    "    return s.strip()    \n",
    "    \n",
    "def check_model(verbose=0,show_training=1):\n",
    "    pred = model.predict(samples)\n",
    "    res = pred.argmax(axis=2)\n",
    "    correct = 0\n",
    "    for i in range(len(pred)):\n",
    "        if(not show_training and i in trainSamples): continue\n",
    "        train=''\n",
    "        if i in trainSamples: train='[T]'\n",
    "        txt = label2words(res[i])\n",
    "        txt_correct = convert(i)\n",
    "        ok=''\n",
    "        if(txt==txt_correct): \n",
    "            correct+=1\n",
    "            ok = \"[ok]\"\n",
    "        if(verbose==1):\n",
    "            print(i,'->',txt, ok,train)\n",
    "    if verbose==0:\n",
    "        for i in range(5):        \n",
    "            x = random.randrange(DATASET_SIZE)\n",
    "            print(x,'->',label2words(res[x]))    \n",
    "    print('Correct',correct,'of',len(pred),' = ',(correct/len(pred)))\n",
    "    return correct,len(pred),(correct/len(pred))\n",
    "\n",
    "\n",
    "EPOCHS=50\n",
    "BATCH_SIZE = int(len(trainSamples)/2)\n",
    "print('Training with',len(trainSamples),'samples',EPOCHS,'epochs and batch_size=',BATCH_SIZE)\n",
    "print(\"Epochs so far\",num_epochs)\n",
    "for x in range(20):\n",
    "    H = model.fit(trainSamples, trainLabels, epochs=EPOCHS,verbose=0,batch_size=BATCH_SIZE)\n",
    "    num_epochs += EPOCHS\n",
    "    print()\n",
    "    print(\"Epoch {} - loss ={:6.3f}, loss improvement ={:6.3f}\".\n",
    "          format(num_epochs,H.history['loss'][-1], H.history['loss'][0]-H.history['loss'][-1]))\n",
    "    pred = model.predict(samples)\n",
    "    res = pred.argmax(axis=2)\n",
    "    c,l,p = check_model()\n",
    "#    print(\"accuracy={:6.3f}%\".format(100*p))\n",
    "#     f = open(\"output.txt\", \"a\")\n",
    "#     f.write(\"=================================================================================\\n\")\n",
    "#     f.write(\"{} Epoch {} - loss ={:6.3f}, loss improvement ={:6.3f}\\n\".\n",
    "#             format(i,num_epochs,H.history['loss'][-1], H.history['loss'][0]-H.history['loss'][-1]))\n",
    "#     f.write(\"accuracy={:6.3f}%\\n\".format(100*p))\n",
    "#     for i in range(len(pred)):\n",
    "#         txt = label2words(res[i])\n",
    "#         f.write(\"{} -> {}\\n\".format(i,txt))\n",
    "#     f.close()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> CCCCCCCCC  \n",
      "1 -> CCCCCCCCC  \n",
      "2 -> XI  \n",
      "3 -> III [ok] [T]\n",
      "4 -> IV [ok] [T]\n",
      "5 -> V [ok] [T]\n",
      "6 -> V  \n",
      "7 -> I  \n",
      "8 -> I  \n",
      "9 -> IX [ok] [T]\n",
      "10 -> X [ok] [T]\n",
      "11 -> XI [ok] \n",
      "12 -> XII [ok] [T]\n",
      "13 -> XIII [ok] [T]\n",
      "14 -> XIV [ok] [T]\n",
      "15 -> XIV  \n",
      "16 -> XIVI  \n",
      "17 -> XVII [ok] [T]\n",
      "18 -> XVIII [ok] [T]\n",
      "19 -> XVIII  \n",
      "20 -> XX [ok] [T]\n",
      "21 -> XXI [ok] [T]\n",
      "22 -> XXII [ok] \n",
      "23 -> XXIVI  [T]\n",
      "24 -> XXIV [ok] [T]\n",
      "25 -> XXIV  \n",
      "26 -> XXIV  \n",
      "27 -> XXIV  \n",
      "28 -> XXIX  \n",
      "29 -> XXIX [ok] [T]\n",
      "30 -> XXX [ok] [T]\n",
      "31 -> XXXI [ok] [T]\n",
      "32 -> XXXII [ok] [T]\n",
      "33 -> XXXII  \n",
      "34 -> XXXV  \n",
      "35 -> XXXV [ok] [T]\n",
      "36 -> XXXVI [ok] [T]\n",
      "37 -> XXXVII [ok] [T]\n",
      "38 -> XXXVII  \n",
      "39 -> XXXVX  [T]\n",
      "40 -> XXXIX  \n",
      "41 -> XLI [ok] [T]\n",
      "42 -> XLI  \n",
      "43 -> XLII  [T]\n",
      "44 -> XLIV [ok] [T]\n",
      "45 -> XLVV  [T]\n",
      "46 -> XLVI [ok] [T]\n",
      "47 -> XLVII [ok] [T]\n",
      "48 -> XLVIII [ok] [T]\n",
      "49 -> XLVIII  \n",
      "50 -> XLV  [T]\n",
      "51 -> L  \n",
      "52 -> LII [ok] [T]\n",
      "53 -> LII  [T]\n",
      "54 -> LII  [T]\n",
      "55 -> LII  \n",
      "56 -> LVI [ok] [T]\n",
      "57 -> LVI  [T]\n",
      "58 -> LVI  \n",
      "59 -> LVI  \n",
      "60 -> LVII  \n",
      "61 -> LVII  \n",
      "62 -> LXII [ok] \n",
      "63 -> LXII  \n",
      "64 -> LXVI  [T]\n",
      "65 -> LXVI  [T]\n",
      "66 -> LXVI [ok] [T]\n",
      "67 -> LXVI  [T]\n",
      "68 -> LXVI  [T]\n",
      "69 -> LXVI  \n",
      "70 -> LXVI  [T]\n",
      "71 -> LXXI [ok] [T]\n",
      "72 -> LXXI  [T]\n",
      "73 -> LXXI  \n",
      "74 -> LXXII  \n",
      "75 -> LXXVI  [T]\n",
      "76 -> LXXVI [ok] [T]\n",
      "77 -> LXXVI  [T]\n",
      "78 -> LXXVI  [T]\n",
      "79 -> LXXVII  [T]\n",
      "80 -> LXXVII  \n",
      "81 -> LXXXII  \n",
      "82 -> LXXXII [ok] [T]\n",
      "83 -> LXXXII  [T]\n",
      "84 -> LXXXII  [T]\n",
      "85 -> LXXXII  [T]\n",
      "86 -> LXXXII  \n",
      "87 -> LXXXVI  [T]\n",
      "88 -> LXXXVI  \n",
      "89 -> LXXXV  \n",
      "90 -> XCXX  [T]\n",
      "91 -> XCI [ok] [T]\n",
      "92 -> XCI  \n",
      "93 -> XCI  [T]\n",
      "94 -> XCI  \n",
      "95 -> XCV [ok] [T]\n",
      "96 -> XCVII  \n",
      "97 -> XCVII [ok] [T]\n",
      "98 -> XCVII  [T]\n",
      "99 -> XCVII  \n",
      "100 -> XCVI  [T]\n",
      "101 -> CCVI  \n",
      "102 -> CCII  \n",
      "103 -> CIII [ok] [T]\n",
      "104 -> CIII  \n",
      "105 -> CVI  \n",
      "106 -> CVI [ok] [T]\n",
      "107 -> CVII [ok] \n",
      "108 -> CVII  [T]\n",
      "109 -> CVII  [T]\n",
      "110 -> CXII  \n",
      "111 -> CXII  \n",
      "112 -> CXII [ok] [T]\n",
      "113 -> CXII  \n",
      "114 -> CXVI  \n",
      "115 -> CXVI  \n",
      "116 -> CXVI [ok] [T]\n",
      "117 -> CXVI  [T]\n",
      "118 -> CXVI  \n",
      "119 -> CXVI  \n",
      "120 -> CXVI  \n",
      "121 -> CXXI [ok] \n",
      "122 -> CXXI  \n",
      "123 -> CXXI  \n",
      "124 -> CXXIV [ok] [T]\n",
      "125 -> CXXIV  [T]\n",
      "126 -> CXXIV  \n",
      "127 -> CXXVI  \n",
      "128 -> CXXXI  \n",
      "129 -> CXXXI  \n",
      "130 -> CXXXII  \n",
      "131 -> CXXXIV  \n",
      "132 -> CXXXII [ok] [T]\n",
      "133 -> CXXXII  [T]\n",
      "134 -> CXXXII  [T]\n",
      "135 -> CXXXII  \n",
      "136 -> CXXXII  [T]\n",
      "137 -> CXXXII  \n",
      "138 -> CXXXII  \n",
      "139 -> CXXXII  \n",
      "140 -> CXXXII  \n",
      "141 -> CXLIII  \n",
      "142 -> CXLIII  \n",
      "143 -> CXLII  [T]\n",
      "144 -> CXLII  [T]\n",
      "145 -> CXLII  [T]\n",
      "146 -> CXLI  \n",
      "147 -> CXLI  \n",
      "148 -> CXLI  [T]\n",
      "149 -> CXLI  \n",
      "150 -> CXLI  \n",
      "151 -> CLLI  [T]\n",
      "152 -> CLLI  \n",
      "153 -> CLII  \n",
      "154 -> CLVI  \n",
      "155 -> CLVI  \n",
      "156 -> CLVI [ok] [T]\n",
      "157 -> CLVI  \n",
      "158 -> CLVII  [T]\n",
      "159 -> CLVII  \n",
      "160 -> CLVII  \n",
      "161 -> CLVII  \n",
      "162 -> CLXII [ok] \n",
      "163 -> CLXII  [T]\n",
      "164 -> CLXII  \n",
      "165 -> CLXII  \n",
      "166 -> CLXII  [T]\n",
      "167 -> CLXVI  \n",
      "168 -> CLXVI  [T]\n",
      "169 -> CLXXI  [T]\n",
      "170 -> CLXXI  [T]\n",
      "171 -> CLXXI [ok] [T]\n",
      "172 -> CLXXI  [T]\n",
      "173 -> CLXXI  [T]\n",
      "174 -> CLXXII  [T]\n",
      "175 -> CLXXII  [T]\n",
      "176 -> CLXXII  [T]\n",
      "177 -> CLXXVI  [T]\n",
      "178 -> CLXXVI  \n",
      "179 -> CLXXVII  \n",
      "180 -> CLXXXII  \n",
      "181 -> CLXXXII  \n",
      "182 -> CLXXXII [ok] [T]\n",
      "183 -> CLXXXII  [T]\n",
      "184 -> CLXXXII  [T]\n",
      "185 -> CLXXXII  [T]\n",
      "186 -> CLXXXII  [T]\n",
      "187 -> CLXXXII  \n",
      "188 -> CLXXXII  \n",
      "189 -> CLXXXI  [T]\n",
      "190 -> CLXXX  \n",
      "191 -> CLXIX  [T]\n",
      "192 -> CLCI  [T]\n",
      "193 -> CLCI  \n",
      "194 -> CLCI  \n",
      "195 -> CLCII  \n",
      "196 -> CLCII  \n",
      "197 -> CLCII  \n",
      "198 -> CLCII  \n",
      "199 -> CLCII  \n",
      "Correct 49 of 200  =  0.245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(49, 200, 0.245)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_model(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wnioski\n",
    "Model ma słabą dokładność. Zmiany w budowie modelu, rozmiarach zbiorów trenujących i testujących i zmiany innych parametrów najczęściej tylko wpływały na bardziej widoczne zjawisko przetrenowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "39d6772f5d9203fca270dcdcddcd2b3571f81c193b04aaa2dedb74f21018db3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
